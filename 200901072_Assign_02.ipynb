{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51026117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression : \" {a -2/5+2} \"\n",
      "\n",
      "Expression after Spliting & Removing Whitesapces : \n",
      "\n",
      " ['{', 'a', '-', '2', '/', '5', '+', '2', '}']\n",
      "\n",
      "Expression after Tokenization : \n",
      "\n",
      " {'{': {'Punctuator'}, 'a': {'Identifier'}, '-': {'Operator'}, '2': {'Constant'}, '/': {'Operator'}, '5': {'Constant'}, '+': {'Operator'}, '}': {'Punctuator'}}\n"
     ]
    }
   ],
   "source": [
    "from pygments.token import Number\n",
    "import re\n",
    "class Lexical_Analyzer:\n",
    "    \n",
    "  def __init__(self,txt):\n",
    "    self.Iinput=txt\n",
    "    self.Tokens=None\n",
    "    self.whiteSpace=' '\n",
    "    self.Operators =['+','*','/']\n",
    "    self.Minus ='-'\n",
    "    self.Punctuators = ['{','}','(',')']\n",
    "    self.special_Characters = ['&','%' ,'$']\n",
    "    self.Tokenization_Of_Expression={}\n",
    "  \n",
    "  def splitFunction(self):\n",
    "  \n",
    "      self.Tokens=[]\n",
    "      for token in self.Iinput:\n",
    "        if token ==  self.whiteSpace:\n",
    "          continue\n",
    "        else:\n",
    "            self.Tokens.append(token)\n",
    "      print('\\nExpression after Spliting & Removing Whitesapces : \\n\\n',self.Tokens)\n",
    "\n",
    "  def print_Before_Tokenization(self):\n",
    "    print( 'Expression : \\\"', self.Iinput,'\\\"')\n",
    "\n",
    "\n",
    "  def  tokenFinder(self):\n",
    "\n",
    "    for finder in self.Tokens:\n",
    "     \n",
    "      if re.search('[a-zA-Z]',finder)  :\n",
    "        self.Tokenization_Of_Expression[finder] ={ 'Identifier'}\n",
    "      elif  re.search(f'{self.Operators}',finder) or finder == self.Minus:\n",
    "          self.Tokenization_Of_Expression[finder] = { 'Operator'}\n",
    "      elif re.search('[0-9]',finder) or finder in Number:\n",
    "          self.Tokenization_Of_Expression[finder]={ 'Constant'}\n",
    "      elif re.search(f'{ self.Punctuators}',finder):\n",
    "          self.Tokenization_Of_Expression[finder]={'Punctuator'}\n",
    "      elif re.search(f'{ self.special_Characters}',finder):\n",
    "          self.Tokenization_Of_Expression[finder]={'Special Character'}\n",
    "   \n",
    "    print( '\\nExpression after Tokenization : \\n\\n',self.Tokenization_Of_Expression)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "  obj = Lexical_Analyzer(\"{a -2/5+2}\")\n",
    "  \n",
    "  obj.print_Before_Tokenization()\n",
    "\n",
    "  obj.splitFunction()\n",
    "\n",
    "  obj.tokenFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f00b93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Expression:3+5\n",
      "\n",
      "Expression : 3+5  \n",
      "\n",
      "\n",
      "Visting Nodes :\n",
      "\n",
      "Expression\n",
      "BinOp\n",
      "Constant\n",
      "Add\n",
      "Constant\n",
      "\n",
      "Interating Fields:\n",
      "\n",
      "body <ast.BinOp object at 0x7fadcd4b03d0>\n",
      "\n",
      "Intering child Nodes:\n",
      "\n",
      "BinOp\n",
      "\n",
      "Dump:\n",
      "\n",
      "Expression(\n",
      "    body=BinOp(\n",
      "        left=Constant(value=3),\n",
      "        op=Add(),\n",
      "        right=Constant(value=5)))\n",
      "\n",
      "Expression after Evaluation : 8\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "class AS_Tree:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Expression = None\n",
    "        self.AsTree = None\n",
    "     \n",
    "    def Input_Equation(self):\n",
    "        self.Expression = input('Enter the Expression:')\n",
    "        print(f'\\nExpression : {self.Expression} ','\\n')\n",
    "        \n",
    "    def Tree_Parser(self):\n",
    "        self.AsTree = ast.parse(self.Expression , mode='eval')\n",
    "  \n",
    "    def inter_nodes(self):\n",
    "      \n",
    "        print('\\nVisting Nodes :\\n')\n",
    "        for node in ast.walk(self.AsTree):\n",
    "            print(node.__class__.__name__)\n",
    "        print('\\nInterating Fields:\\n')\n",
    "        for name , value in ast.iter_fields(self.AsTree):\n",
    "            print(name , value)\n",
    "        print('\\nIntering child Nodes:\\n')\n",
    "        for node in ast.iter_child_nodes(self.AsTree):\n",
    "            print(node.__class__.__name__)\n",
    "       \n",
    "            \n",
    "    def Dump_ast(self):\n",
    "        print('\\nDump:\\n')\n",
    "        print(ast.dump(self.AsTree, indent=4))\n",
    "    \n",
    "    def Evale_ASTree(self):\n",
    "        print('\\nExpression after Evaluation :',eval(compile(self.AsTree,'',mode='eval')) )\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    ASTree = AS_Tree()\n",
    "    ASTree.Input_Equation()\n",
    "    ASTree.Tree_Parser()\n",
    "    ASTree.inter_nodes()\n",
    "    ASTree.Dump_ast()\n",
    "    ASTree.Evale_ASTree()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ced902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
